# MySQL Performance Tuning ‚Äì Practice Repository

This repository documents my **learning journey**, **hands-on practice**, and **experiments**
while following the YouTube playlist:

üé• **MySQL Performance Tuning**

üîó Playlist Link:  
https://www.youtube.com/playlist?list=PLBrWqg4Ny6vXQZqsJ8qRLGRH9osEa45sw

---

## üéØ Purpose of This Repository

The goal of this repository is to **deeply understand MySQL performance tuning**
through real experiments, large datasets, and practical query analysis.

This is **not theory only** ‚Äî every concept is:

- Implemented
- Tested
- Measured
- Documented

---

## üìå Learning Objectives

- Understand how MySQL executes queries internally
- Learn how the MySQL Optimizer works
- Analyze queries using `EXPLAIN` and `EXPLAIN ANALYZE`
- Practice indexing strategies and measure their impact
- Generate large datasets (1M / 5M rows) for performance testing
- Apply real-world performance tuning techniques

---

## üõ† Initial Setup & Preparation

Before starting the actual performance analysis topics, I focused on setting up
a solid foundation that can be reused throughout the learning process.

---

### üì¶ Database Preparation

- Created a reusable table structure to simulate a real-world users table
- The table includes:
  - User identity information
  - Location-related fields
  - Account-related attributes
- This table will be used in upcoming performance and indexing experiments

---

### üß™ Data Volume Strategy

- Decided to work with large datasets to make performance behavior visible
- Planned data sizes:
  - 1,000,000 rows
  - 5,000,000 rows
- This approach helps reveal:
  - Differences between indexed and non-indexed queries
  - Full table scans vs index scans
  - Optimizer decision patterns

---

### ‚öôÔ∏è Data Generation Approach

- Prepared a stored procedure to generate large volumes of test data
- Data is inserted in batches to balance performance and safety
- Generated data includes:
  - Unique user records
  - Different account types
  - Simulated geographic distribution
- This setup allows repeating experiments consistently

---

### üß† Learning Mindset

- Focus on understanding **why** a query behaves in a certain way
- Avoid premature optimization
- Rely on measurements and execution plans instead of assumptions

---

## üßæ Initial SQL Queries & Setup Scripts

As part of the preparation phase, the following SQL queries and scripts were written
to set up a controlled environment for future performance testing.

These queries are not focused on optimization yet,
but are essential for creating realistic testing conditions.

---

### üóÑ Table Creation Query

The following query was used to create a reusable table structure
that simulates a real-world users table.

```sql
CREATE TABLE user_info (
  id              INT UNSIGNED NOT NULL AUTO_INCREMENT,
  name            VARCHAR(64)  NOT NULL DEFAULT '',
  email           VARCHAR(64)  NOT NULL DEFAULT '',
  password        VARCHAR(64)  NOT NULL DEFAULT '',
  dob             DATE DEFAULT NULL,
  address         VARCHAR(255) NOT NULL DEFAULT '',
  city            VARCHAR(64)  NOT NULL DEFAULT '',
  state_id        SMALLINT UNSIGNED NOT NULL DEFAULT '0',
  zip             VARCHAR(8)   NOT NULL DEFAULT '',
  country_id      SMALLINT UNSIGNED NOT NULL DEFAULT '0',
  account_type    VARCHAR(32)  NOT NULL DEFAULT '',
  closest_airport VARCHAR(3)   NOT NULL DEFAULT '',
  PRIMARY KEY (id),
  UNIQUE KEY email (email)
) ENGINE=InnoDB;
```

---

### üî¢ Creating the Helper Numbers Table (`seq_10000`)

To efficiently generate large datasets, a helper numbers table was created.

This table provides a sequence of numbers that can be reused in multiple
data generation and performance testing scenarios.

The main goal of this table is to avoid inserting rows one by one
and instead rely on set-based operations.

```sql

CREATE TABLE seq_10000 (
  n INT NOT NULL PRIMARY KEY
) ENGINE=InnoDB;

INSERT INTO seq_10000 (n)
SELECT
  (a.i + b.i * 10 + c.i * 100 + d.i * 1000) + 1 AS n
FROM
  (SELECT 0 i UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4
   UNION ALL SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9) a
CROSS JOIN
  (SELECT 0 i UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4
   UNION ALL SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9) b
CROSS JOIN
  (SELECT 0 i UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4
   UNION ALL SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9) c
CROSS JOIN
  (SELECT 0 i UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4
   UNION ALL SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9) d;
```

---

### ‚öôÔ∏è Stored Procedure for Bulk Data Generation

A stored procedure was created to generate large datasets
in a controlled and repeatable way.

This procedure supports:

- Inserting millions of rows
- Batch-based inserts
- Transaction control
- Repeatable test data generation

```sql
DELIMITER $$

CREATE PROCEDURE seed_user_info(IN p_total_rows INT, IN p_batch_size INT)
BEGIN
  DECLARE v_inserted INT DEFAULT 0;
  DECLARE v_take INT DEFAULT 0;

  IF p_total_rows <= 0 THEN
    SIGNAL SQLSTATE '45000'
      SET MESSAGE_TEXT = 'p_total_rows must be > 0';
  END IF;

  IF p_batch_size <= 0 OR p_batch_size > 10000 THEN
    SIGNAL SQLSTATE '45000'
      SET MESSAGE_TEXT = 'p_batch_size must be between 1 and 10000';
  END IF;

  SET autocommit = 0;

  WHILE v_inserted < p_total_rows DO
    SET v_take = LEAST(p_batch_size, p_total_rows - v_inserted);

    INSERT INTO user_info
      (name, email, password, dob, address, city, state_id, zip, country_id, account_type, closest_airport)
    SELECT
      CONCAT('User_', v_inserted + s.n),
      CONCAT('user', v_inserted + s.n, '@example.com'),
      CONCAT('pass_', v_inserted + s.n),
      DATE_ADD('1985-01-01', INTERVAL ((v_inserted + s.n) % 12000) DAY),
      CONCAT('Address ', v_inserted + s.n),
      CONCAT('City_', ((v_inserted + s.n) % 200)),
      ((v_inserted + s.n) % 50),
      LPAD(((v_inserted + s.n) % 99999), 5, '0'),
      ((v_inserted + s.n) % 250),
      ELT(((v_inserted + s.n) % 3) + 1, 'basic', 'premium', 'business'),
      ELT(((v_inserted + s.n) % 5) + 1, 'DXB', 'RUH', 'JED', 'AMM', 'DOH')
    FROM seq_10000 s
    WHERE s.n <= v_take;

    SET v_inserted = v_inserted + v_take;
    COMMIT;
  END WHILE;

  SET autocommit = 1;
END$$

DELIMITER ;
```

---

### üìä Data Seeding & Verification Queries

After preparing the table structure and the data generation procedure,
the following queries were used to populate and verify the dataset.

```sql
TRUNCATE TABLE user_info;

-- Generate 5 million records
CALL seed_user_info(5000000, 10000);
```

Verification queries:

```sql
SELECT COUNT(*) FROM user_info;

SELECT *
FROM user_info
ORDER BY id DESC
LIMIT 5;
```

---

## üîç EXPLAIN ANALYZE ‚Äì Indexing Experiments (COUNT with filters)

In this experiment, I tested how MySQL behaves when running the same query
under different indexing strategies.

Query under test:

```sql
EXPLAIN ANALYZE
SELECT COUNT(*)
FROM user_info
WHERE `name` = 'User_1000' AND state_id = 0;
```

---

### 1Ô∏è‚É£ Baseline ‚Äì No Index

```sql
EXPLAIN ANALYZE
SELECT COUNT(*)
FROM user_info
WHERE `name` = 'User_1000' AND state_id = 0;
```

Execution plan:

```text
-> Aggregate: count(0)  (cost=532807 rows=1) (actual time=955..955 rows=1 loops=1)
    -> Filter: ((user_info.state_id = 0) and (user_info.`name` = 'User_1000'))
        (cost=521626 rows=48525) (actual time=2.39..955 rows=1 loops=1)
        -> Table scan on user_info
           (cost=521626 rows=4.85e+6) (actual time=0.77..838 rows=5e+6 loops=1)
```

Notes:

- MySQL performed a full table scan.
- Around 5 million rows were scanned.
- This represents the slowest possible scenario.

---

### 2Ô∏è‚É£ Index on `state_id`

```sql
ALTER TABLE user_info ADD INDEX state_id_idx(state_id);

EXPLAIN ANALYZE
SELECT COUNT(*)
FROM user_info
WHERE `name` = 'User_1000' AND state_id = 0;
```

Execution plan:

```text
-> Aggregate: count(0)  (cost=119641 rows=1) (actual time=280..280 rows=1 loops=1)
    -> Filter: (user_info.`name` = 'User_1000')
        (cost=114802 rows=21002) (actual time=1.34..280 rows=1 loops=1)
        -> Index lookup on user_info using state_id_idx (state_id = 0)
           (cost=114802 rows=210022) (actual time=1.31..273 rows=100000 loops=1)
```

Notes:

- MySQL used the `state_id_idx` index to narrow down rows.
- Filtering by `name` happened after the index lookup.
- Performance improved compared to full table scan, but many rows were still examined.

---

### 3Ô∏è‚É£ Index on `name` (Single-column Index)

```sql
ALTER TABLE user_info ADD INDEX name_idx(name);

EXPLAIN ANALYZE
SELECT COUNT(*)
FROM user_info
WHERE `name` = 'User_1000' AND state_id = 0;
```

Execution plan:

```text
-> Aggregate: count(0)  (cost=1.01 rows=1) (actual time=0.252..0.252 rows=1 loops=1)
    -> Filter: (user_info.state_id = 0)
        (cost=1 rows=0.05) (actual time=0.244..0.246 rows=1 loops=1)
        -> Index lookup on user_info using name_idx (name = 'User_1000')
           (cost=1 rows=1) (actual time=0.241..0.243 rows=1 loops=1)
```

Notes:

- MySQL chose the `name_idx` index due to high selectivity.
- Only one row was retrieved from the index.
- `state_id` was checked after fetching the row.
- Performance improved significantly.

---

### 4Ô∏è‚É£ Composite Index on (`name`, `state_id`)

```sql
DROP INDEX name_idx ON user_info;
DROP INDEX state_id_idx ON user_info;

ALTER TABLE user_info ADD INDEX name_state_id_idx(name, state_id);

EXPLAIN ANALYZE
SELECT COUNT(*)
FROM user_info
WHERE `name` = 'User_1000' AND state_id = 0;
```

Execution plan:

```text
-> Aggregate: count(0)  (cost=1.33 rows=1) (actual time=0.0972..0.0974 rows=1 loops=1)
    -> Covering index lookup on user_info using name_state_id_idx
       (name = 'User_1000', state_id = 0)
       (cost=1.1 rows=1) (actual time=0.0837..0.0885 rows=1 loops=1)
```

Notes:

- MySQL used a covering index lookup.
- Both conditions were satisfied directly from the index.
- No additional table row lookup was needed.
- This was the fastest execution among all scenarios.

---

### ‚úÖ Summary of Observations

- No index ‚Üí full table scan (slowest).
- Index on `state_id` ‚Üí partial improvement, still scans many rows.
- Index on `name` ‚Üí very fast due to high selectivity.
- Composite index (`name`, `state_id`) ‚Üí best result for this query pattern,
  using a covering index and minimal row access.

---

## üî¨ Query Profiling Using Performance Schema & sys Schema

Beyond analyzing single queries with `EXPLAIN ANALYZE`,
it is critical to understand **how queries behave over time** under real workloads.

MySQL provides two powerful internal schemas for this purpose:

- `performance_schema` ‚Üí low-level execution statistics
- `sys` ‚Üí human-readable views built on top of `performance_schema`

These tools allow identifying:

- The most expensive queries overall
- Queries executed too frequently
- Full table scans and missing indexes
- Lock contention and I/O pressure

##

### üìä Identifying Top Time-Consuming Queries

To detect the queries responsible for most of the database load,
we analyze aggregated execution statistics using
`events_statements_summary_by_digest`.

Queries are normalized (digest-based), allowing us to group
logically identical queries that differ only by literal values.

```sql
SELECT
  (100 * SUM_TIMER_WAIT / SUM(SUM_TIMER_WAIT) OVER()) AS percent,
  SUM_TIMER_WAIT AS total,
  COUNT_STAR AS calls,
  AVG_TIMER_WAIT AS mean,
  SUBSTRING(DIGEST_TEXT, 1, 75) AS digest_preview,
  DIGEST_TEXT
FROM performance_schema.events_statements_summary_by_digest
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;
```

---

### Interpretation of the columns

**Key columns explained:**

| Column           | Description                                           |
| ---------------- | ----------------------------------------------------- |
| `DIGEST_TEXT`    | Normalized form of the SQL query                      |
| `COUNT_STAR`     | Number of times the query was executed                |
| `SUM_TIMER_WAIT` | Total accumulated execution time                      |
| `AVG_TIMER_WAIT` | Average execution time per call                       |
| `percent`        | Percentage contribution to total query execution time |

> üîç **Insight**
>
> A query does not need to be slow to be expensive.
> A frequently executed fast query can consume more total time
> than a rarely executed slow one.

##

### üîí Lock Contention & Temporary Table Analysis

Additional performance indicators help identify deeper problems
related to locking, sorting, and grouping behavior.

| Column                        | Meaning                             | When to Investigate                    |
| ----------------------------- | ----------------------------------- | -------------------------------------- |
| `SUM_LOCK_TIME`               | Total time spent waiting for locks  | High contention or long transactions   |
| `SUM_CREATED_TMP_TABLES`      | Temp tables created (memory + disk) | Heavy GROUP BY / ORDER BY              |
| `SUM_CREATED_TMP_DISK_TABLES` | Temp tables created on disk         | Insufficient memory or missing indexes |
| `SUM_SORT_MERGE_PASSES`       | Number of sort merge passes         | Sort buffer too small                  |
| `SUM_ROWS_EXAMINED`           | Rows scanned by the query           | Inefficient access path                |
| `SUM_ROWS_SENT`               | Rows returned to client             | Large result sets                      |

##

### üö® Detecting Full Table Scans

The `sys` schema provides simplified views for identifying
queries and tables that rely on full table scans.

```sql
SELECT *
FROM sys.statements_with_full_table_scans
ORDER BY no_index_used_count DESC;
```

```sql
SELECT *
FROM sys.schema_tables_with_full_table_scans;
```

> ‚ö†Ô∏è **Warning**
>
> Frequent full table scans on large tables often indicate
> missing or ineffective indexes.

##

### üìà Index Usage Analysis

To verify whether indexes are actually being used,
we inspect index-level I/O statistics.

```sql
SELECT
  object_schema,
  object_name,
  index_name,
  count_star
FROM performance_schema.table_io_waits_summary_by_index_usage
WHERE object_name = 'user_info';
```

**Interpretation:**

- High `count_star` ‚Üí index heavily used
- Low or NULL `index_name` ‚Üí possible table scans

##

### üìÇ Table I/O Activity

Table-level I/O statistics reveal how often rows are read and fetched.

```sql
SELECT *
FROM performance_schema.table_io_waits_summary_by_table
WHERE object_name = 'user_info';
```

Key indicators:

- `COUNT_READ` ‚Üí number of read operations
- `COUNT_FETCH` ‚Üí number of row fetches

##

### ‚ùå Error & Deadlock Monitoring

```sql
SELECT *
FROM performance_schema.events_errors_summary_by_account_by_error
WHERE error_name = 'er_lock_deadlock';
```


This helps detect concurrency issues such as:
- Deadlocks
- Conflicting transaction order

## ‚úÖ Key Takeaways

- Performance issues must be analyzed over time, not per query only
- High-frequency queries can dominate total execution time
- Temporary disk tables are a strong signal of suboptimal queries
- Index existence does not guarantee index usage
- `performance_schema` + `sys` provide production-grade observability
